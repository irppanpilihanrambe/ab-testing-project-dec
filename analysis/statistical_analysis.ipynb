{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9m/3r36_gr96hd50nvzq0zh85h40000gn/T/ipykernel_57975/222624045.py:17: UserWarning: Validation module not available. Skipping validation checks.\n",
      "  warnings.warn(\"Validation module not available. Skipping validation checks.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required sample size per variant: 31,231\n",
      "\n",
      "\n",
      "  Normality Check for 'Revenue per User':\n",
      "    Control: Non-normal\n",
      "    Treatment: Non-normal\n",
      "    → Using Mann-Whitney U test (non-parametric)\n",
      "\n",
      "================================================================================\n",
      "MULTIPLE TESTING CORRECTION (Per Experiment)\n",
      "================================================================================\n",
      "Number of tests conducted: 4\n",
      "Correction method: Holm-Bonferroni\n",
      "Uncorrected significant: 1/3\n",
      "Corrected significant: 1/3\n",
      "FWER without correction: 0.143\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Test Results:\n",
      "================================================================================\n",
      "             metric                test_type        pvalue  significant  \\\n",
      "0   Conversion Rate          proportion_test  9.397606e-02        False   \n",
      "1   Conversion Rate               chi_square  1.065734e-01        False   \n",
      "2  Revenue per User             mann_whitney  9.554408e-08         True   \n",
      "3  Revenue per User  welch_t_test_comparison  2.501073e-06         True   \n",
      "\n",
      "   relative_lift_pct  \n",
      "0          21.317829  \n",
      "1                NaN  \n",
      "2                NaN  \n",
      "3           9.680834  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "\n",
    "try:\n",
    "    from validation import ExperimentValidator\n",
    "    VALIDATION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    VALIDATION_AVAILABLE = False\n",
    "    warnings.warn(\"Validation module not available. Skipping validation checks.\")\n",
    "\n",
    "\n",
    "class ABTestAnalyzer:\n",
    "    \n",
    "    def __init__(self, alpha: float = 0.05):\n",
    "        self.alpha = alpha\n",
    "        if VALIDATION_AVAILABLE:\n",
    "            self.validator = ExperimentValidator(srm_threshold=0.001)  \n",
    "        else:\n",
    "            self.validator = None\n",
    "    \n",
    "    def calculate_sample_size(self,\n",
    "                            baseline_rate: float,\n",
    "                            mde: float,\n",
    "                            alpha: float = 0.05,\n",
    "                            power: float = 0.80,\n",
    "                            two_tailed: bool = True) -> int:\n",
    "        \n",
    "        # Critical values\n",
    "        if two_tailed:\n",
    "            z_alpha = stats.norm.ppf(1 - alpha/2)\n",
    "        else:\n",
    "            z_alpha = stats.norm.ppf(1 - alpha)\n",
    "        \n",
    "        z_beta = stats.norm.ppf(power)\n",
    "        \n",
    "        # Effect size\n",
    "        p1 = baseline_rate\n",
    "        p2 = baseline_rate * (1 + mde)\n",
    "        \n",
    "        # Ensure p2 is valid probability\n",
    "        p2 = min(p2, 0.999)\n",
    "        \n",
    "        # Sample size calculation\n",
    "        numerator = (z_alpha + z_beta) ** 2 * (p1 * (1 - p1) + p2 * (1 - p2))\n",
    "        denominator = (p2 - p1) ** 2\n",
    "        \n",
    "        n = numerator / denominator\n",
    "        \n",
    "        return int(np.ceil(n))\n",
    "    \n",
    "    def two_sample_ttest(self,\n",
    "                        control: np.ndarray,\n",
    "                        treatment: np.ndarray,\n",
    "                        metric_name: str,\n",
    "                        equal_var: bool = False) -> Dict:\n",
    "        \n",
    "        # Remove NaN values\n",
    "        control = control[~np.isnan(control)]\n",
    "        treatment = treatment[~np.isnan(treatment)]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        control_mean = control.mean()\n",
    "        treatment_mean = treatment.mean()\n",
    "        control_std = control.std(ddof=1)\n",
    "        treatment_std = treatment.std(ddof=1)\n",
    "        n_control = len(control)\n",
    "        n_treatment = len(treatment)\n",
    "        \n",
    "        # Perform t-test\n",
    "        statistic, pvalue = stats.ttest_ind(treatment, control, equal_var=equal_var)\n",
    "        \n",
    "        # Calculate Cohen's d (effect size)\n",
    "        # Reference: Cohen, J. (1988)\n",
    "        pooled_std = np.sqrt((control_std**2 + treatment_std**2) / 2)\n",
    "        cohens_d = (treatment_mean - control_mean) / pooled_std if pooled_std > 0 else 0\n",
    "        \n",
    "        # Calculate confidence interval for difference in means\n",
    "        # Using Welch's approach for unequal variances\n",
    "        se_diff = np.sqrt(control_std**2/n_control + treatment_std**2/n_treatment)\n",
    "        \n",
    "        # Degrees of freedom (Welch-Satterthwaite equation)\n",
    "        if not equal_var:\n",
    "            num = (control_std**2/n_control + treatment_std**2/n_treatment)**2\n",
    "            denom = ((control_std**2/n_control)**2/(n_control-1) + \n",
    "                    (treatment_std**2/n_treatment)**2/(n_treatment-1))\n",
    "            df = num / denom if denom > 0 else n_control + n_treatment - 2\n",
    "        else:\n",
    "            df = n_control + n_treatment - 2\n",
    "        \n",
    "        t_crit = stats.t.ppf(1 - self.alpha/2, df)\n",
    "        diff = treatment_mean - control_mean\n",
    "        ci_lower = diff - t_crit * se_diff\n",
    "        ci_upper = diff + t_crit * se_diff\n",
    "        \n",
    "        # Relative lift percentage\n",
    "        relative_lift_pct = (diff / control_mean * 100) if control_mean != 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'metric': metric_name,\n",
    "            'test_type': 't-test',\n",
    "            'statistic': statistic,\n",
    "            'pvalue': pvalue,\n",
    "            'significant': pvalue < self.alpha,\n",
    "            'control_mean': control_mean,\n",
    "            'treatment_mean': treatment_mean,\n",
    "            'control_std': control_std,\n",
    "            'treatment_std': treatment_std,\n",
    "            'absolute_diff': diff,\n",
    "            'relative_lift_pct': relative_lift_pct,\n",
    "            'cohens_d': cohens_d,\n",
    "            'effect_interpretation': self._interpret_cohens_d(cohens_d),\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'n_control': n_control,\n",
    "            'n_treatment': n_treatment,\n",
    "            'degrees_of_freedom': df\n",
    "        }\n",
    "    \n",
    "    def proportion_test(self,\n",
    "                       control_successes: int,\n",
    "                       control_total: int,\n",
    "                       treatment_successes: int,\n",
    "                       treatment_total: int,\n",
    "                       metric_name: str) -> Dict:\n",
    "        \n",
    "        # Calculate proportions\n",
    "        p_control = control_successes / control_total\n",
    "        p_treatment = treatment_successes / treatment_total\n",
    "        \n",
    "        # Pooled proportion\n",
    "        p_pooled = (control_successes + treatment_successes) / (control_total + treatment_total)\n",
    "        \n",
    "        # Standard error\n",
    "        se = np.sqrt(p_pooled * (1 - p_pooled) * (1/control_total + 1/treatment_total))\n",
    "        \n",
    "        # Z-statistic\n",
    "        z_stat = (p_treatment - p_control) / se if se > 0 else 0\n",
    "        \n",
    "        # P-value (two-tailed)\n",
    "        pvalue = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "        \n",
    "        # Confidence interval for difference\n",
    "        se_diff = np.sqrt(p_control*(1-p_control)/control_total + \n",
    "                         p_treatment*(1-p_treatment)/treatment_total)\n",
    "        z_crit = stats.norm.ppf(1 - self.alpha/2)\n",
    "        diff = p_treatment - p_control\n",
    "        ci_lower = diff - z_crit * se_diff\n",
    "        ci_upper = diff + z_crit * se_diff\n",
    "        \n",
    "        # Relative lift\n",
    "        relative_lift_pct = (diff / p_control * 100) if p_control > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'metric': metric_name,\n",
    "            'test_type': 'proportion_test',\n",
    "            'statistic': z_stat,\n",
    "            'pvalue': pvalue,\n",
    "            'significant': pvalue < self.alpha,\n",
    "            'control_rate': p_control,\n",
    "            'treatment_rate': p_treatment,\n",
    "            'absolute_diff': diff,\n",
    "            'relative_lift_pct': relative_lift_pct,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'n_control': control_total,\n",
    "            'n_treatment': treatment_total\n",
    "        }\n",
    "    \n",
    "    def chi_square_test(self,\n",
    "                       control: np.ndarray,\n",
    "                       treatment: np.ndarray,\n",
    "                       metric_name: str) -> Dict:\n",
    "        \n",
    "        # Create contingency table\n",
    "        combined = np.concatenate([control, treatment])\n",
    "        labels = np.concatenate([np.zeros(len(control)), np.ones(len(treatment))])\n",
    "        \n",
    "        contingency_table = pd.crosstab(combined, labels)\n",
    "        \n",
    "        # Chi-square test\n",
    "        chi2, pvalue, dof, expected = chi2_contingency(contingency_table)\n",
    "        \n",
    "        # Cramér's V effect size\n",
    "        # Reference: Cramér, H. (1946)\n",
    "        n = len(combined)\n",
    "        min_dim = min(contingency_table.shape[0], contingency_table.shape[1]) - 1\n",
    "        cramers_v = np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'metric': metric_name,\n",
    "            'test_type': 'chi_square',\n",
    "            'statistic': chi2,\n",
    "            'pvalue': pvalue,\n",
    "            'significant': pvalue < self.alpha,\n",
    "            'degrees_of_freedom': dof,\n",
    "            'cramers_v': cramers_v,\n",
    "            'effect_interpretation': self._interpret_cramers_v(cramers_v),\n",
    "            'n_control': len(control),\n",
    "            'n_treatment': len(treatment)\n",
    "        }\n",
    "    \n",
    "    def mann_whitney_u_test(self,\n",
    "                           control: np.ndarray,\n",
    "                           treatment: np.ndarray,\n",
    "                           metric_name: str) -> Dict:\n",
    "        \n",
    "        # Remove NaN\n",
    "        control = control[~np.isnan(control)]\n",
    "        treatment = treatment[~np.isnan(treatment)]\n",
    "        \n",
    "        # Mann-Whitney U test\n",
    "        statistic, pvalue = stats.mannwhitneyu(treatment, control, alternative='two-sided')\n",
    "        \n",
    "        # Rank-biserial correlation (effect size for Mann-Whitney)\n",
    "        # r = 1 - (2U)/(n₁×n₂)\n",
    "        n1 = len(control)\n",
    "        n2 = len(treatment)\n",
    "        rank_biserial = 1 - (2*statistic) / (n1 * n2)\n",
    "        \n",
    "        # Medians for interpretation\n",
    "        control_median = np.median(control)\n",
    "        treatment_median = np.median(treatment)\n",
    "        \n",
    "        return {\n",
    "            'metric': metric_name,\n",
    "            'test_type': 'mann_whitney',\n",
    "            'statistic': statistic,\n",
    "            'pvalue': pvalue,\n",
    "            'significant': pvalue < self.alpha,\n",
    "            'control_median': control_median,\n",
    "            'treatment_median': treatment_median,\n",
    "            'rank_biserial': rank_biserial,\n",
    "            'n_control': n1,\n",
    "            'n_treatment': n2\n",
    "        }\n",
    "    \n",
    "    def bootstrap_confidence_interval(self,\n",
    "                                     control: np.ndarray,\n",
    "                                     treatment: np.ndarray,\n",
    "                                     metric_name: str,\n",
    "                                     n_bootstrap: int = 10000,\n",
    "                                     confidence_level: float = 0.95) -> Dict:\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Remove NaN\n",
    "        control = control[~np.isnan(control)]\n",
    "        treatment = treatment[~np.isnan(treatment)]\n",
    "        \n",
    "        # Bootstrap\n",
    "        boot_diffs = []\n",
    "        for _ in range(n_bootstrap):\n",
    "            control_boot = np.random.choice(control, size=len(control), replace=True)\n",
    "            treatment_boot = np.random.choice(treatment, size=len(treatment), replace=True)\n",
    "            boot_diffs.append(treatment_boot.mean() - control_boot.mean())\n",
    "        \n",
    "        boot_diffs = np.array(boot_diffs)\n",
    "        \n",
    "        # Percentile method\n",
    "        alpha_bootstrap = 1 - confidence_level\n",
    "        ci_lower = np.percentile(boot_diffs, alpha_bootstrap/2 * 100)\n",
    "        ci_upper = np.percentile(boot_diffs, (1 - alpha_bootstrap/2) * 100)\n",
    "        \n",
    "        # Point estimate\n",
    "        observed_diff = treatment.mean() - control.mean()\n",
    "        \n",
    "        # Significance (if CI excludes 0)\n",
    "        significant = not (ci_lower <= 0 <= ci_upper)\n",
    "        \n",
    "        return {\n",
    "            'metric': metric_name,\n",
    "            'test_type': 'bootstrap',\n",
    "            'observed_diff': observed_diff,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'significant': significant,\n",
    "            'confidence_level': confidence_level,\n",
    "            'n_bootstrap': n_bootstrap\n",
    "        }\n",
    "    \n",
    "    def multiple_testing_correction(self,\n",
    "                                   p_values: List[float],\n",
    "                                   method: str = 'holm') -> Dict:\n",
    "        \n",
    "        # Use statsmodels for correction\n",
    "        reject, pvals_corrected, alphacSidak, alphacBonf = multipletests(\n",
    "            p_values, \n",
    "            alpha=self.alpha, \n",
    "            method=method\n",
    "        )\n",
    "        \n",
    "        # Family-Wise Error Rate without correction\n",
    "        fwer_uncorrected = 1 - (1 - self.alpha) ** len(p_values)\n",
    "        \n",
    "        return {\n",
    "            'method': method,\n",
    "            'original_pvalues': p_values,\n",
    "            'corrected_pvalues': pvals_corrected.tolist(),\n",
    "            'reject': reject.tolist(),\n",
    "            'fwer_uncorrected': fwer_uncorrected,\n",
    "            'num_tests': len(p_values),\n",
    "            'num_significant_uncorrected': sum(p < self.alpha for p in p_values),\n",
    "            'num_significant_corrected': sum(reject)\n",
    "        }\n",
    "    \n",
    "    def _check_normality(self, data: np.ndarray, sample_size: int = 5000) -> bool:\n",
    "        data_clean = data[~np.isnan(data)]\n",
    "        \n",
    "        if len(data_clean) == 0:\n",
    "            return False\n",
    "        \n",
    "        # Sample if too large (Shapiro-Wilk limit is 5000)\n",
    "        if len(data_clean) > sample_size:\n",
    "            np.random.seed(42)\n",
    "            sample = np.random.choice(data_clean, size=sample_size, replace=False)\n",
    "        else:\n",
    "            sample = data_clean\n",
    "        \n",
    "        # Shapiro-Wilk test\n",
    "        # H0: data comes from normal distribution\n",
    "        # If p < 0.05, reject H0 (data is NOT normal)\n",
    "        _, p_value = stats.shapiro(sample)\n",
    "        \n",
    "        return p_value >= 0.05  # Normal if p >= 0.05\n",
    "    \n",
    "    def analyze_test(self,\n",
    "                    df: pd.DataFrame,\n",
    "                    variant_col: str,\n",
    "                    metrics: Dict,\n",
    "                    run_validation: bool = True) -> pd.DataFrame:\n",
    "        \n",
    "        # Run validation if available and requested\n",
    "        if run_validation and self.validator is not None:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"RUNNING EXPERIMENTAL VALIDATION CHECKS\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # Detect covariates (non-metric columns that could be confounders)\n",
    "            covariates = []\n",
    "            for col in df.columns:\n",
    "                if col not in [variant_col] + [m['column'] for m in metrics.values()]:\n",
    "                    if df[col].dtype in [np.int64, np.float64] or isinstance(df[col].dtype, pd.CategoricalDtype):\n",
    "                        # Check if binary or low cardinality\n",
    "                        if df[col].nunique() <= 10:\n",
    "                            covariates.append(col)\n",
    "            \n",
    "            # SRM Test\n",
    "            print(\"\\n1. Sample Ratio Mismatch Test\")\n",
    "            print(\"-\" * 80)\n",
    "            srm_result = self.validator.sample_ratio_mismatch_test(df, variant_col)\n",
    "            print(srm_result.get('message', srm_result.get('warning', '')))\n",
    "            \n",
    "            if srm_result['has_srm']:\n",
    "                raise ValueError(\n",
    "                    \"⚠️ CRITICAL: Sample Ratio Mismatch detected. \"\n",
    "                    \"Experiment is INVALID. Do not proceed with analysis.\"\n",
    "                )\n",
    "            \n",
    "            # Covariate balance if covariates exist\n",
    "            if covariates:\n",
    "                print(\"\\n2. Covariate Balance Check\")\n",
    "                print(\"-\" * 80)\n",
    "                balance_result = self.validator.covariate_balance_check(\n",
    "                    df, variant_col, covariates\n",
    "                )\n",
    "                print(balance_result.get('message', ''))\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"VALIDATION COMPLETE - PROCEEDING WITH ANALYSIS\")\n",
    "            print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        # Get unique variants\n",
    "        variants = df[variant_col].unique()\n",
    "        \n",
    "        if len(variants) != 2:\n",
    "            warnings.warn(f\"Found {len(variants)} variants. Analysis designed for 2-arm tests.\")\n",
    "        \n",
    "        # Assume first variant is control\n",
    "        control_variant = sorted(variants)[0]\n",
    "        treatment_variant = sorted(variants)[1]\n",
    "        \n",
    "        control_df = df[df[variant_col] == control_variant]\n",
    "        treatment_df = df[df[variant_col] == treatment_variant]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for metric_name, config in metrics.items():\n",
    "            column = config['column']\n",
    "            metric_type = config['type']\n",
    "            \n",
    "            # Extract data\n",
    "            control_data = control_df[column].values\n",
    "            treatment_data = treatment_df[column].values\n",
    "            \n",
    "            # Select appropriate test based on metric type\n",
    "            if metric_type == 'binary':\n",
    "                # Proportion test\n",
    "                control_successes = int(control_data.sum())\n",
    "                treatment_successes = int(treatment_data.sum())\n",
    "                \n",
    "                result = self.proportion_test(\n",
    "                    control_successes, len(control_data),\n",
    "                    treatment_successes, len(treatment_data),\n",
    "                    metric_name\n",
    "                )\n",
    "                results.append(result)\n",
    "                \n",
    "                # Also run chi-square as alternative\n",
    "                result_chi = self.chi_square_test(control_data, treatment_data, metric_name)\n",
    "                results.append(result_chi)\n",
    "                \n",
    "            elif metric_type == 'continuous':\n",
    "                # Check normality to decide which test to use\n",
    "                control_normal = self._check_normality(control_data)\n",
    "                treatment_normal = self._check_normality(treatment_data)\n",
    "                \n",
    "                print(f\"\\n  Normality Check for '{metric_name}':\")\n",
    "                print(f\"    Control: {'Normal' if control_normal else 'Non-normal'}\")\n",
    "                print(f\"    Treatment: {'Normal' if treatment_normal else 'Non-normal'}\")\n",
    "                \n",
    "                if control_normal and treatment_normal:\n",
    "                    # Both normal → Use Welch's t-test\n",
    "                    print(f\"    → Using Welch's t-test (parametric)\")\n",
    "                    result = self.two_sample_ttest(control_data, treatment_data, metric_name)\n",
    "                    result['normality_check'] = 'normal_distribution'\n",
    "                    result['test_selection'] = 'automatic_parametric'\n",
    "                    results.append(result)\n",
    "                else:\n",
    "                    # Non-normal → Use Mann-Whitney U (primary)\n",
    "                    print(f\"    → Using Mann-Whitney U test (non-parametric)\")\n",
    "                    result = self.mann_whitney_u_test(control_data, treatment_data, metric_name)\n",
    "                    result['normality_check'] = 'non_normal_distribution'\n",
    "                    result['test_selection'] = 'automatic_nonparametric'\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Also include t-test for comparison (with warning)\n",
    "                    result_t = self.two_sample_ttest(control_data, treatment_data, metric_name)\n",
    "                    result_t['test_type'] = 'welch_t_test_comparison'\n",
    "                    result_t['normality_check'] = 'non_normal_distribution'\n",
    "                    result_t['warning'] = 'Data non-normal; prefer Mann-Whitney result above'\n",
    "                    result_t['test_selection'] = 'comparison_only'\n",
    "                    results.append(result_t)\n",
    "        \n",
    "        # Apply multiple testing correction within this experiment\n",
    "        if len(results) > 1:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"MULTIPLE TESTING CORRECTION (Per Experiment)\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Number of tests conducted: {len(results)}\")\n",
    "            \n",
    "            # Extract p-values (only from primary tests, not comparison tests)\n",
    "            primary_results = [r for r in results if r.get('test_selection') != 'comparison_only']\n",
    "            p_values = [r['pvalue'] for r in primary_results if 'pvalue' in r]\n",
    "            \n",
    "            if len(p_values) > 1:\n",
    "                # Apply Holm-Bonferroni correction\n",
    "                correction_result = self.multiple_testing_correction(\n",
    "                    p_values, \n",
    "                    method='holm'\n",
    "                )\n",
    "                \n",
    "                print(f\"Correction method: Holm-Bonferroni\")\n",
    "                print(f\"Uncorrected significant: {correction_result['num_significant_uncorrected']}/{len(p_values)}\")\n",
    "                print(f\"Corrected significant: {correction_result['num_significant_corrected']}/{len(p_values)}\")\n",
    "                print(f\"FWER without correction: {correction_result['fwer_uncorrected']:.3f}\")\n",
    "                \n",
    "                # Update primary results with corrected p-values\n",
    "                p_idx = 0\n",
    "                for result in primary_results:\n",
    "                    if 'pvalue' in result:\n",
    "                        result['pvalue_corrected'] = correction_result['corrected_pvalues'][p_idx]\n",
    "                        result['significant_corrected'] = correction_result['reject'][p_idx]\n",
    "                        result['correction_method'] = 'holm'\n",
    "                        result['num_tests_corrected'] = len(p_values)\n",
    "                        p_idx += 1\n",
    "                \n",
    "                print(f\"{'='*80}\\n\")\n",
    "            else:\n",
    "                print(\"Only 1 primary test - no correction needed\")\n",
    "                print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    # Helper methods\n",
    "    def _interpret_cohens_d(self, d: float) -> str:\n",
    "        \"\"\"Interpret Cohen's d effect size.\"\"\"\n",
    "        abs_d = abs(d)\n",
    "        if abs_d < 0.2:\n",
    "            return \"Negligible\"\n",
    "        elif abs_d < 0.5:\n",
    "            return \"Small\"\n",
    "        elif abs_d < 0.8:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"Large\"\n",
    "    \n",
    "    def _interpret_cramers_v(self, v: float) -> str:\n",
    "        \"\"\"Interpret Cramér's V effect size.\"\"\"\n",
    "        if v < 0.1:\n",
    "            return \"Negligible\"\n",
    "        elif v < 0.3:\n",
    "            return \"Small\"\n",
    "        elif v < 0.5:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"Large\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Sample size calculation\n",
    "    analyzer = ABTestAnalyzer()\n",
    "    \n",
    "    required_n = analyzer.calculate_sample_size(\n",
    "        baseline_rate=0.05,\n",
    "        mde=0.10,\n",
    "        power=0.80\n",
    "    )\n",
    "    print(f\"Required sample size per variant: {required_n:,}\\n\")\n",
    "    \n",
    "    # Generate example data\n",
    "    n = 5000\n",
    "    df = pd.DataFrame({\n",
    "        'variant': np.random.choice(['control', 'treatment'], n),\n",
    "        'conversion': np.random.binomial(1, 0.05, n),\n",
    "        'revenue': np.random.gamma(2, 3, n),\n",
    "        'device_type': np.random.choice(['mobile', 'desktop'], n)\n",
    "    })\n",
    "    \n",
    "    # Add treatment effect\n",
    "    treatment_mask = df['variant'] == 'treatment'\n",
    "    df.loc[treatment_mask, 'conversion'] = np.random.binomial(1, 0.055, treatment_mask.sum())\n",
    "    df.loc[treatment_mask, 'revenue'] = np.random.gamma(2.2, 3, treatment_mask.sum())\n",
    "    \n",
    "    # Define metrics\n",
    "    metrics = {\n",
    "        'Conversion Rate': {'column': 'conversion', 'type': 'binary'},\n",
    "        'Revenue per User': {'column': 'revenue', 'type': 'continuous'}\n",
    "    }\n",
    "    \n",
    "    # Analyze\n",
    "    results = analyzer.analyze_test(df, 'variant', metrics, run_validation=True)\n",
    "    \n",
    "    print(\"\\nTest Results:\")\n",
    "    print(\"=\"*80)\n",
    "    print(results[['metric', 'test_type', 'pvalue', 'significant', 'relative_lift_pct']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (Anaconda)",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
